{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Крок 1: Завантаження бази знань\n",
    "\n",
    "Використовуємо бібліотеку `datasets` від Hugging Face для завантаження даних. Цей крок дозволяє нам отримати доступ до специфічної бази даних, яка вже поділена на частини для навчання та тестування. Дані містять текст документів, які будемо використовувати для подальшої обробки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "page_content_column = \"context\"\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d936271e8c4befab178528552aed64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aef50673ed245419517d3630abe3e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdef9cfd171f43dab99be5bec5c8cc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}, page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia\\'s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"'),\n",
       " Document(metadata={'instruction': 'Which is a species of fish? Tope or Rope', 'response': 'Tope', 'category': 'classification'}, page_content='\"\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loader.load()\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Крок 2: Розділення документів\n",
    "\n",
    "Для ефективного вилучення інформації та зручності у подальшому пошуку, документи потрібно розділити на менші частини. Це дозволяє системі швидше знаходити відповідні фрагменти тексту, які відповідають запитам користувача. Використовуємо `RecursiveCharacterTextSplitter` для членування текстів на сегменти, що забезпечує кращу маневреність та точність при пошуку відповідей на запити."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Крок 3: Створення векторної бази даних та налаштування вкладень\n",
    "Після розділення тексту на чанки, кожен чанк перетворюється на вектор за допомогою моделі вкладень. Ці вектори зберігаються в базі даних FAISS для швидкого векторного пошуку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Крок 4: Встановлення системи вилучення інформації та генерація відповідей\n",
    "Останній крок - налаштування системи для вилучення інформації та генерації текстових відповідей на запити користувача. Використовуючи модель для генерації тексту, система аналізує контекст, вилучений з бази даних, і формулює відповідь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "device = 0 if torch.cuda.is_available() else -1 \n",
    "\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"Intel/dynamic_tinybert\",\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\"),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital city of the US\"\n",
    "qa_inputs = {\n",
    "    \"question\": question,\n",
    "    \"context\": retriever.get_relevant_documents(question)[0].page_content\n",
    "}\n",
    "\n",
    "result = question_answerer(qa_inputs)\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
